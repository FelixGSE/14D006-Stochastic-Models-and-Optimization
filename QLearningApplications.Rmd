#Functioning of the code
This section describes the code contained in the Appendix. The code is largely based on the implementation of the [Innohead team](sites.google.com/a/innohead.com/wiki/Home/Python/reinforcement-learning---tic-tac-toe0). Though, we adjusted some parameters and added others to augment the depth of our experimental analysis. ALso we enhanced the GUI (graphical user interface).

The general structure of the code is organized around classes. They are Python structures containing both functions and objects that can belong to the class' environement or to the global environment. Classes allow for functions and variables which are closely related to each other to be stores in one place and thus interact more easily. 

The class **State** is creating the actual board with the visual representaion of the values ( _ , X, and O) and their corresponding tuple keys recognized by the learner algorithm (No Play, Player 1, Player 2). The object takes as input the current actions of the players and fills the table accordingly. It also contains two additional functions. One of them returns whether one of the players won the game and the other one the game checks if the board is full. 

The class **Learner** is the most important in the code. It contains the function which enumerates all available actions for a player at a given state. Another function in this class evaluates the gain of a particular state. In particular, the reward of a win is 1, of a loss is -1, and of a tie is 0. The function next_action is pivot as it determines what will be the next move of the AI. The next move corresponds to the action which, according to the past experience, has the highest value, a function of the value. The value is computed by another part of the code. More precisely, the value of a state is updating using the value of the next state after evaluating the result of the last action. We have also enriched the original code by adding a parameter allowing more or less for randomness in the 'next action' decision process. Whe end-up with the standard Q-learning iteration:
$$value(lastState) = (1-\alpha)intermediateValue(lastState) + \alpha value(newState)$$

The class **Game** is in charge of the display of the information to the human player. This is also where we choose to let the AI to train against itself and we can specify for how many games as well. Owning to this class we can also record where the players choose to play in the board.

The class **Selfplay** creates the oponent player and make two AIs play against each other. The aim is to prepare the AI to play (well) against a human player. We also use this class to evaluate how an AI is performing and how its sequences of actions are converging to the optimal strategy with different sets of parameters. We discuss that in the next section.

While playing the game, the human user is allowed to train the AI with N games with the function g.selfplay(N), where N is a natural number. To play the game call the function g(i,j) which enable you to position your mark on the tic-tac-toe board - i is the row and j is the column.

#Q-learning applications review

##Classical Q-learning
In this section, we present similar variations of the Q-learning algorithm for other applications.

###Pacman
The Pacman AI describes a state using a vector of features. The vector actually capture multiple different states simultaneously. The vector entries are real numbers, often 0/1, which are calculated by functions that capture important properties of the state: distance to closest ghost, distance to closest dot, number of ghosts, $1/(distance to dot)^2$, is Pacman in a tunnel? (0/1 example). This information is used to find which actions usually result in good outcomes, and which result in bad. So if in the current state there is a food pellet above the AI, and the AI moves up to eat the food pellet, it will remember that, in this state, moving up and eating the food pellet was a good decision because its score increased. As in the tic-tac-toe exanple, the game is played repeatedly to build a table of memories so the robot gets better. 

###Financial trading
It is possible to automatically evaluate the best course of n actions among buying, selling, and staying out (do not do anything). This again is done using algorithm's experience and without any prior knowledge of the environment in advance. The Q-learning algorithm is achieved by optimizing different value functions: internal profit, sharp  ratio,  derivative  sharp  ratio. The selection of the right value functions is important to achieve a stable learning algorithm, especially if the environement is unstable. Also, we add auxilary parameter in order to learn the value function - the variance of the environment's variables. It is reported by Du et al. (2009) that this approach leads to better performance in the algorithm  even than the performance achieved using stationary and fixed value functions. 

###Intelligent anti-tank mines
Mine-carying robots have been designed with sensors determining enemy tanks' direction and velocity (the state). Based on this, two perceptual triggers (actions) are initialized: CAN_INTERCEPT and NEAR. If CAN_INTERCEPT is TRUE, then the robot chooses the action INTERCEPT. If NEAR is TRUE, it chooses the action TERMINATE. Otherwise it chooses WAIT. The robot is rewarded if a tank is destroyed after a termination action. The perceptual triggers are reevaluated accordingly to the history of rewards. 

##Q-learning with approximations
Q-learning might loose viability with applications having too large state sizes or action spaces. To solve this problem, some applications use function approximations speeding up the learning by providing finite state spaces. There is even possible to work in infinite state spaces.

###Backgammon
Artificial neural networks are an example of function approximation. They are implemented in Tesauro's backgammon player using TD learning. It is useful for non-deterministic game with smooth and continuous state space where similar board positions have similar values. In contrast, in deterministic games, such as chess, a small difference in board position can have large consequences for the state value. 

###Deepmind
A recent application by Google DeepMind merged Q-learning with deep learning. Titled "deep reinforcement learning" or "deep Q-networks", it was able to beat humans on Atari games. An astonishing fact is that the agent, actually receiving only the pixel information and the game score as inputs, was able to surpass the performance of all previous algorithms. Actually it achieves a level comparable to that of a professional human tester across a set of 49 games, using the same algorithm, network architecture and hyperparameters. 

The Q-network algorithm combines both the successes of the Q-learning policy research and the Deep Learning ability to map high-dimentional inputs to simpler spaces. In particular, for each game play, the network has as many loss functions as there are potential actions to reach the objective state. Also, to avoid getting stuck in loops and/or local minima, the loss functions are evaluated as the averaged value of different (and not always optimal in a greedy perspective) tries.

#References
http://cs229.stanford.edu/proj2009/LvDuZhai.pdf

http://www.cc.gatech.edu/ai/robot-lab/online-publications/iros02-ebeowulf-saho-arkin.pdf

http://www.readcube.com/articles/10.1038%2Fnature14236
