#Functioning of the code
This section describes the code contained in the Appendix. This code is largely based on the implementation of the (Innohead team)[sites.google.com/a/innohead.com/wiki/Home/Python/reinforcement-learning---tic-tac-toe]. Although, we adjusted some parameters and added others to augment the depth of our experimental analysis. We also enhanced the GUI.

The general structure of the code is organized around classes. These are Python structure containing functions and objects that can belong to the class' environement or to the global environment.

The class State is creating the actual game board with values as they are displayed (_, X, and O) and the corresponding values that are recognized by the learner algorithm (No Play, Player 1, Player 2). For this, it takes as input the current actions of the players and fills the table accordingly. It also contains the functions returning whether one of the players won the game and one looking if the board is full. 

The class Learner is the most important in the code. It contains the function enumerating the possible actions left available for a player at a given state. It also contains the function evaluating the gain of a particular state. In particular, the gain of a win is 1, a loss is 0, and an intermediate state is 0.1. The function next_action is pivotal as it determines what will be the next move of the AI. The next move corresponds to the action which, according to past experience, has the highest value, a function of the gain. The latter is computed by another part of the code; more precisely, the value of the previous state is set after evaluating the result of the last action. Compared to the original code, we also added the a parameter allowing for more or less randomness is 'next action' decision process. When end-up with the standard Q-learning iteration:
$$value(lastState) = (1-\alpha)intermediateValue(lastState) + \alpha value(newState)$$

The class Game is in charge of the display of the information to the human player. This is also where we choose to let the AI to train against itself and for how many games. This is also thanks to this class that we record where the players choose to play.

The class Selfplay creates the oponent player and make two AIs play against each other. The aim is to prepare the AI to play (well) against a human player. We also use this class to evaluate how an AI is performing and how its sequences of actions are converging with different sets of parameters. We discuss that in the next section.

While playing the game, the human user is allowed to train the AI with N games with the function g.selfplay(N). Typically after this first step, it is possible to play the game with the call g(i,j) where i is the row of a tic-tac-toe board, and j is the column.

#Q-learning applications review

##Classical Q-learning
In this section, we present similar variations of the Q-learning algorithm for other applications.

###Pacman
The Pacman AI describes a state using a vector of features. These are functions from states to real numbers (often 0/1) that capture important properties of the state: distance to closest ghost, distance to closest dot, number of ghosts, $1/(distance to dot)^2$, is Pacman in a tunnel? (0/1). This information is used to find which actions usually result in good outcomes, and which result in bad. So if in the current state there is a food pellet above the AI, and the AI moves up to eat the food pellet, it will remember that, in this state, moving up was a good decision because its score increased. The game is played repeatedly to build a table of memories so the robot gets better. 

###Financial trading
It is possible to automatically evaluate the best course of n actions among buying, selling, and staying out. This is done from experience and without knowing the environment in advance. For this, different value functions can be optimized: internal profit, sharp  ratio,  derivative  sharp  ratio. The selection of the right value function is important to achieve a stable learning algorithm, especially applied when the environement is unstable. Also, learning in parallel the variance of environment's variables is reported by Du et al. (2009) to lead to better performance in the algorithm than that with stationary and fixed value functions. 

###Intelligent anti-tank mines
Mine-carying robots have been designed with sensors determining enemy tanks' direction and velocity. Based on this, two perceptual triggers are initialized: CAN_INTERCEPT and NEAR. If CAN_INTERCEPT is TRUE, the robot chooses the action INTERCEPT. If NEAR is TRUE, it chooses the action TERMINATE. Otherwise it chooses WAIT. The robot is rewarded if a tank is destroyed after a termination action. The perceptual triggers are reevaluated accordingly to the history of rewards. 

##Q-learning with approximations
Q-learning might loose viability with applications having too large state sizes or action spaces. To solve this problem, some applications use function approximations speeding up the learning in finite state spaces. It even allows to work in infinite state spaces.

###Backgammon
Artificial neural networks as a function approximator are an example of function approximation. They are implemented in Tesauro's backgammon player using temporal difference learning, an unsupervised technique in which the agent learns to predict the expected value of a variable occurring at the end of a sequence of states. It is useful for such non-deterministic game with smooth and continuous state space where similar board positions have similar values. In deterministic games, such as chess, a small difference in board position can have large consequences for the state value. 

###Deepmind
A recent application by Google DeepMind merged Q-learning with deep learning. Titled "deep reinforcement learning" or "deep Q-networks", it was able to beat humans on Atari games. An astonishing fact is that the agent, actually receiving only the pixels and the game score as inputs, was able to surpass the performance of all previous algorithms and achieve a level comparable to that of a professional human tester across a set of 49 games, using the same algorithm, network architecture and hyperparameters. 

The algorithm uses a deep Q-network, that, mimicking humans, uses Q-learning to learn successful policies directly from high-dimensional sensory inputs, thanks to Deep learning. In particular, for each round, the network has as many loss functions as there are potential actions to reach the objective state. Also, to avoid getting stuck in loops and/or local minima, the loss functions are evaluated as the averaged value of different (and not always optimal in a greedy perspective) tries.

#References
http://cs229.stanford.edu/proj2009/LvDuZhai.pdf

http://www.cc.gatech.edu/ai/robot-lab/online-publications/iros02-ebeowulf-saho-arkin.pdf

http://www.readcube.com/articles/10.1038%2Fnature14236
