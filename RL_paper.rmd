---
title: "Reinforcement Learning in it's Tic-Tac-Toe Implementation"
author: "Felix Gutmann, Thomas Vicente, Zsuzsa Holler and Denitsa Panova"
date: "March 31, 2016"
output: html_document
---

#Introduction

>“If we choose to call the former [Chess-Player] a pure machine we must be prepared to 
>admit that it is, beyond all comparison, the most wonderful of the inventions of mankind.”
>Edgar Allan Poe, Maelzel’s Chess-Player (c. 1830) 

Poe's words remind us that men have strived to create artificial intelligence (AI) long before we actually had the computing power to do so. Today almost all computing boundaries are gone. We have witnessed a lot of algorithms which question which ideas are in the realm of science fiction and which are actually achievable in the near future. The latest accomplishment is the Google AI which beats one of the top GO players in the world. (a task considered impossible before) 

In this paper we concentrate on one of the main technique which is used to create AIs – reinforcement learning (RL),  and more specifically on Q-learning. Moreover, we focus on the RL in board games.  But why board games? What is so special about them? The answer is simple – we use them as measure for the ability to implement abstract thinking. They are sign of intelligence and the capacity to evolve and learn form your ups and downs. So it is only fare to ask if computers can be up to the challenge. Although for humans it is natural to learn to master games and consequently shine with their intelligence, for machine this is not the case. There are many computational methods which have been created in order to “teach” the AI how to play successfully. In order to elaborate more on the topic we focus on RL and its implementation to one of the oldest games in the world – tic-tac-toe. The game has been around for three and half millennia and nowadays it is typically popular among kids. On 3 by 3 board players alternate terns to put either X or O. A player wins if there is either row, column or diagonal filled with his/her symbol. 

#Structure of the Paper 

#Reinforcement Learning (RL)

##Introduction

One of the main aims of machine learning is to create intelligent algorithm (agents). Reinforcement learning is one of the ways to achieve this goal. Let's compare it to supervised learning. The agent in supervised learning is taught how to react in a given situation. On the contrary, in RL the agent is not taught. He has the free will what to do whatever he wants (more or less random choice). When the action is completed, the agent is told if it has been a good or bad decision by providing negaive or positive reward. He learns how he should act in the future based on that constant learning. Put in other words the agent is learning from interacting with its environment and observing what is the outcome of this interaction. However, we should note that the agent is not told if his action is good or bad immediately after it has been made. The reward comes at the end, after several additional steps. This leads to the so-called called “temporal credit assignment problem:” how do we decide which step contributes the most to the success of a game, if the agent does series of decisions before getting the reward . We deal this problem using RL and especially temporal difference learning. We will elaborate more on the topic later on.

##Reasoning 

The reasoning for the RL is simple. Imagine playing the game. Adults use reasoning and analysis to deduce which is the best step. However, children do not. They mostly use the trail-error approach. This is exactly what RL does. When children play they see that they put an X in a specific box which led to success. Therefore, they put an X more often there. For them this box is associated with winning not with the appropriate contraction to the opponent's move. By playing a lot the kid or the agent learns which are the “winning” boxes and the “cause and effect idea” is achieved. If we need to translate this to a programing language, the RL looks the following:  
1. Agent observes the state (each state is a decision)  
2. An action is undertaken by a decision policy (a function which defines agent's action in a given state)  
3. The agent receives a scalar reward or reinforcement - a function of state and action (the agent maximizes total reward over a long horizon)  
4. Information about the reward in the particular state is recorded  

If long enough is played (through the whole life cycle), then the agent will have enough information to perform perfectly in the environment.    

##Exploitation and Exploration Trade-off

One of the most crucial trade-offs which RL faces is the one between exploitation and exploration. If the agent has performed a specific action in the past and has received a positive reward, then most likely he will prefer it in case it is an option again (exploitation of the acquired knowledge). However, this prevents the algorithm from finding new paths which could possible be better rewarded (exploration of new information). So the balance between both aspects is, indeed tremendous. The most common way to achieve it, is by trying  different actions and at the same time favoring those that give the biggest reward. Put in other words, we randomize in order not to get stuck in a local maximum/minimum.

##Value Function
The value functions are the key to solve the “temporal credit assignment problem” which we have discussed previously. They depend on the state-action pair which estimates how good a particular action will be in a given state, or what the return for that action is expected to be. The value functions indicate the long-run premium in contrast to the reward which is the immediate gain of an action. The following notations are used:  
- $s$- state  
- $\pi$ – policy  
- $V_\pi(s)$ – the **value** of state $s$ and policy $\pi$  
- $Q_\pi(s,a)$ is the **expected reward** if we undertake action $a$ and if we are at state $s$ and have policy $\pi$.  
The next question which needs to be answered is: how to estimate those value functions in order to understand which is the best reward and consequently which is the best action to take.  
##Temporal Difference Learning
Temporal Difference Learning (TD) is one of the possible ways to estimate them. TD is "a combination of Monte Carlo ideas and dynamic programming (DP) ideas." Its Monte Carlo aspect is that the algorithm learns by sampling the environment according to a policy, and the dynamic programming side is corresponding to approximating the current estimate by previously learned estimates.  There are two types of TD based on when the reward is estimated– at the end of the game or at each state.  
**Estimating at each state:** $V(s_t) ← V(s_t) +\alpha[r_t+1 +\gammaV(s_t+1) -V(s_t)]$  
**Estimating at the final stage:** $V(s_t) ← V(s_t) +\alpha[V(s_t+1) - V(s_t)] $    
where $s_t$ is the current stage;   
$s_t+1$ is the next stage;   
$\alpha$ is the learning rate ( a number between 0 and 1 which shows the extent to which the newly acquired information will override the old information);  
$\gamma$ is the discount rate (importance of future rewards, representing idea that future rewards worth less than the current ones );  
$r_{t+1}$ is the observed reward at time t+1   
Since the game we have chosen is tic-tac-toe (receives a reward at the end of the game), we concentrate on the the latter.  
##The Difference between On- and Off-Policy
The On-Policy is used to learn the **value of a particular policy**. The update is done by values of the same policy. The policies are usually “soft”, meaning that they ensure exploration is exploited. This is done by adding randomness to the system. The algorithm chooses random action (not-reward-optimizing one) with probability epsilon. 
The Off-Policy learns different policies in order to **evaluate behavior and value estimation.** Again the policies should be “soft” in order to promote exploration. Note that in the On-Policy method the update of the estimated value is done strictly based on **experience**, whereas in the Off-Policy approach hypothetical actions are also used. This means that an agent trained by an off-policy method may end up learning strategies that it has not necessarily seen during the learning phase.
##Q-learning
Q-learning is a type of Off-Policy TD method. It is model-free approach to find the $Q$ value function. It can be proven that give sufficiently many “soft” policies it can converge with probability 1 to the action-state value function for an arbitrary policy if the following criteria are met:  
- each action is executed in each state an infinite number of times;  
- $\alpha$ is decreased with an appropriate schedule;  
- action-values are stored perfectly.  
Moreover, if this approach is used, it is guaranteed that the optimal policy is reached even when actions are selected according to a more exploratory or even random policy.   
**The Q-algorithm**:  
1. Observe the current state, $s$.  
2. Choose an action, $a$, for the particular state based on one of the policies   
3. Take the action, and observe the reward, $r$, as well as the new state, $s'$.  
4. Update the Q-value for the state using the observed reward and the maximum reward possible for the next state. The updating is in the following way:  
$Q(s,a) ←  Q(s,a) + \alpha(r + \gamma max_\alpha[Q(s',a') – Q(s,a)]$   
where $\alpha$ is the learning rate, $\gamma$ is the discount factor and $max_\alpha$ is the maximum reward in the following state.   
5. Set the state to the new state, and repeat the process until a terminal state is reached.  
In our case (the tic-tac-toe game) we don't have reward for every action. Therefore, the value functions coincide. 

##Q-learning as Dynamic Programming Algorithm 
We know that the $Q$ value function of each action in a particular state can be expressed as a sum of the rewards of an action in the particular state plus the expected future rewards if we continue using the same policy.  
$Q(s,a) = E[ r_t + \gamma r_t+1(s_{t+1}, argmaxQ(s_{t+1}), _{t+2}) + \gamma^2r_{t+2}(s_{t+2}, argmaxQ(s_{t+1}), _{t+2}) + … ]   
If we want to maximize equation , we can transform the problem in such which is looking for $Q*$ (Watkins, 1989; Watkins and Dayan, 1992).  The DP approach find $Q*$ iteratively through a forward dynamic model.   
$Q*(s,a) ← r + \gamma max_\alpha[Q(s,a)]$ in $\alpha$ proportion  
where ← is a move-forward operator. This means that the left-hand-side is moved to the right-hand-side in proportion $\alpha$. Therefore, convergence is guaranteed with probability 1.  
## Other Q-learning Applications
#The Tic-Tac-Toe Application
#References 
Richard Sutton and Andrew Barto (1998). Reinforcement Learning. MIT Press. ISBN 0-585-02445-6  
Peng Ding and Tao Mao, Reinforcement Learning in Tic-Tac-Toe Game and Its Similar Variations, Thayer School of Engineering at Dartmouth College  
Sutton, Richard S. and Barto, Andrew G., Reinforcement Learning: An Introduction, MIT Press, 1998  
The Reinforcement Learning Repository, University of Massachusetts, Amherst  
Tesauro, Gerald, Temporal Difference Learning and TD-Gammon, Communications of the Association for Computing Machinery, March 1995 / Vol 38, No. 3  
Imran Ghory, Reinforcement learning in board games, 2004   
Chris Gaskett, Q-learning for Robot Control, 2002
# Appendix